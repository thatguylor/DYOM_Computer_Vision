# -*- coding: utf-8 -*-
"""Copy of Flickr Style Dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11h03ruIXPXKU_SoyqiC4U4WEKkFWKwmp
"""

from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
import tensorflow.keras as keras
from tensorflow.keras import models
from tensorflow.keras import layers
from tensorflow.keras import optimizers
import tensorflow as tf
from keras.utils import np_utils
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing import image
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import cv2 
import random
from sklearn.metrics import plot_confusion_matrix
from tensorflow.keras.models import model_from_json
from matplotlib import pyplot
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from keras.preprocessing.image import array_to_img
from google.colab import files
from io import BytesIO
from skimage.transform import resize
from keras.preprocessing.image import ImageDataGenerator

###Hyperparameter tuning 
BATCH_SIZE = 32
EPOCHS = 5
LEARNING_RATE = 2e-5

# create generator
datagen = ImageDataGenerator(rescale=1./255) 

#Initialize class names from Flickr Style Dataset 
class_names = ["Bokeh","Romance","Depth of Field","Detailed","Ethereal","Geometric Composition","Hazy","HDR","Horror","Long Exposure"] 

# prepare an iterators for each dataset
train_it = datagen.flow_from_directory('/content/drive/My Drive/Data/Train',target_size=(320,320), class_mode='categorical', batch_size=BATCH_SIZE, interpolation = 'nearest')
test_it = datagen.flow_from_directory('/content/drive/My Drive/Data/Test',target_size=(320,320), class_mode='categorical',batch_size= BATCH_SIZE, interpolation = 'nearest')
# confirm the iterator works
batchX, batchy = train_it.next() 
testX, testy = test_it.next()
print('Batch shape=%s, min=%.3f, max=%.3f' % (batchX.shape, batchX.min(), batchX.max()))   
print(testX.shape) 
print(batchy[1])

rand = random.randint(0,BATCH_SIZE)
plt.imshow(batchX[rand]) 
plt.xlabel(class_names[np.argmax(batchy[rand])])

conv_base = ResNet50(weights='imagenet', include_top=False, input_shape=(320,320,3))

STEP_SIZE_TRAIN=train_it.n//train_it.batch_size
STEP_SIZE_TEST=test_it.n//test_it.batch_size

model = models.Sequential()
#model.add(layers.UpSampling2D((2,2)))
#model.add(layers.UpSampling2D((2,2)))
#model.add(layers.UpSampling2D((2,2))) 
model.add(conv_base)
model.add(layers.Flatten())
model.add(layers.BatchNormalization())
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dropout(0.5))
model.add(layers.BatchNormalization())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dropout(0.5))
model.add(layers.BatchNormalization())
#model.add(layers.Dense(64, activation='relu')) ##These may be uncommented to add more layers to CNN 
#model.add(layers.Dropout(0.5)) ##These may be uncommented to add more layers to CNN
#model.add(layers.BatchNormalization()) ##These may be uncommented to add more layers to CNN
model.add(layers.Dense(10, activation='softmax'))

model.compile(optimizer=optimizers.RMSprop(lr=LEARNING_RATE), loss='binary_crossentropy', metrics=['acc'])

history = model.fit_generator(generator=train_it,
                    steps_per_epoch=STEP_SIZE_TRAIN,
                    validation_data=test_it,
                    validation_steps=STEP_SIZE_TEST,
                    epochs=EPOCHS)

#save model weights to drive
model_json = model.to_json()
with open('model.json', 'w') as json_file:
    json_file.write(model_json)
model.save_weights('/content/drive/My Drive/model.h5')

model.summary()

model.evaluate_generator(generator=test_it,
steps=STEP_SIZE_TEST)  
print(model.metrics_names)

history_dict = history.history
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']

epochs = range(1, len(loss_values) + 1)

plt.figure(figsize=(14, 4))

plt.subplot(1,2,1)
plt.plot(epochs, loss_values, 'bo', label='Training Loss')
plt.plot(epochs, val_loss_values, 'b', label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

acc = history_dict['acc']
val_acc = history_dict['val_acc']

epochs = range(1, len(loss_values) + 1)

plt.subplot(1,2,2)
plt.plot(epochs, acc, 'bo', label='Training Accuracy', c='orange')
plt.plot(epochs, val_acc, 'b', label='Validation Accuracy', c='orange')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

num_images = 20 #Change this to display more images in multiples of 5 
#train_it.reset()
#test_it.reset()
plt.figure(figsize=(25,25))
for i in range(num_images):
    plt.subplot(num_images/5,5,i+1)
    #plt.xticks([])
    #plt.yticks([])
    plt.grid(False)
    plt.imshow(testX[i], cmap=plt.cm.binary) #Change this from test to train
    plt.xlabel(class_names[np.argmax(testy[i])]) #Change this from test to train
plt.show()

# Load trained CNN model
json_file = open('model.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
model = model_from_json(loaded_model_json)
model.load_weights('/content/drive/My Drive/model.h5')

indices = np.argmax(model.predict_on_batch(testX[:num_images]),1) #Change this from test to train but use x_train or x_test
print("Predicted labels for test images displayed above:")
print([class_names[x] for x in indices]) 

lst1 = []
a= 0

for i in range(num_images):
  lst1.append(class_names[np.argmax(testy[i])])  #Change this from test to train
lst2 = [class_names[x] for x in indices]

for i in range(num_images): 
  if lst2[i] == lst1[i]: 
    a+=1
  else: continue
print("CNN Model can predict", a ,"correctly out of", num_images ,"test images.")

top_k = 3 #Change this value for more k 
n = random.randint(0,BATCH_SIZE) #Change this number to display any of the above nth images
lst = [] 
idx = []

top_values, top_indices = tf.nn.top_k(model.predict_on_batch(testX[n-1:n]), k=top_k) 

plt.imshow(testX[n], cmap=plt.cm.binary) #Change this from test to train
plt.xlabel(class_names[np.argmax(testy[n])])
plt.show()  

for i in range(0,top_k): 
  lst.append(class_names[top_indices.numpy()[0][i]])
  idx.append(top_values.numpy()[0][i])


objects = tuple(lst)
y_pos = np.arange(len(objects))
performance = idx
plt.bar(y_pos, performance, align='center', alpha=0.5)
plt.xticks(y_pos, objects)
plt.ylabel('Probability')
plt.title('Top K Classes')
plt.show()

path = '/content/drive/My Drive/horror1.jpg' ##Change this for image path
image = tf.keras.preprocessing.image.load_img(
    path, grayscale=False, color_mode='rgb', target_size=(320,320),
    interpolation='nearest')  
plt.imshow(image)

array = tf.keras.preprocessing.image.img_to_array(image, data_format=None, dtype=None)
array /= 255 
array = np.resize(array,(1,320,320,3)) 

top_k = 3 #Change to display more K classes

lst1=[]
lst2=[]

pred = np.argmax(model.predict_on_batch(array[0:1]),1) 
print("Predicted class for uploaded image is:",str([class_names[x] for x in pred]))

top_values, top_indices = tf.nn.top_k(model.predict_on_batch(array[0:1]), k=top_k) 

for i in range(0,top_k): 
  lst1.append(class_names[top_indices.numpy()[0][i]])
  lst2.append(top_values.numpy()[0][i])


objects = tuple(lst1)
y_pos = np.arange(len(objects))
performance = lst2
plt.bar(y_pos, performance, align='center', alpha=0.5)
plt.xticks(y_pos, objects)
plt.ylabel('Probability')
plt.title('Top K Classes')
plt.show()